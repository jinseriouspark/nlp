{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처 : https://huggingface.co/transformers/_modules/transformers/configuration_bert.html#BertConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-902447a58a63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" BERT model configuration \"\"\"\n",
    "\n",
    "#from .configuration_utils import PretrainedConfig\n",
    "#from .utils import logging\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "# 미리 학습된 config 파일을 확인할 수 있음 \n",
    "BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
    "    \"bert-base-uncased\": \"https://huggingface.co/bert-base-uncased/resolve/main/config.json\",\n",
    "    \"bert-large-uncased\": \"https://huggingface.co/bert-large-uncased/resolve/main/config.json\",\n",
    "    \"bert-base-cased\": \"https://huggingface.co/bert-base-cased/resolve/main/config.json\",\n",
    "    \"bert-large-cased\": \"https://huggingface.co/bert-large-cased/resolve/main/config.json\",\n",
    "    \"bert-base-multilingual-uncased\": \"https://huggingface.co/bert-base-multilingual-uncased/resolve/main/config.json\",\n",
    "    \"bert-base-multilingual-cased\": \"https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json\",\n",
    "    \"bert-base-chinese\": \"https://huggingface.co/bert-base-chinese/resolve/main/config.json\",\n",
    "    \"bert-base-german-cased\": \"https://huggingface.co/bert-base-german-cased/resolve/main/config.json\",\n",
    "    \"bert-large-uncased-whole-word-masking\": \"https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json\",\n",
    "    \"bert-large-cased-whole-word-masking\": \"https://huggingface.co/bert-large-cased-whole-word-masking/resolve/main/config.json\",\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\": \"https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json\",\n",
    "    \"bert-large-cased-whole-word-masking-finetuned-squad\": \"https://huggingface.co/bert-large-cased-whole-word-masking-finetuned-squad/resolve/main/config.json\",\n",
    "    \"bert-base-cased-finetuned-mrpc\": \"https://huggingface.co/bert-base-cased-finetuned-mrpc/resolve/main/config.json\",\n",
    "    \"bert-base-german-dbmdz-cased\": \"https://huggingface.co/bert-base-german-dbmdz-cased/resolve/main/config.json\",\n",
    "    \"bert-base-german-dbmdz-uncased\": \"https://huggingface.co/bert-base-german-dbmdz-uncased/resolve/main/config.json\",\n",
    "    \"cl-tohoku/bert-base-japanese\": \"https://huggingface.co/cl-tohoku/bert-base-japanese/resolve/main/config.json\",\n",
    "    \"cl-tohoku/bert-base-japanese-whole-word-masking\": \"https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json\",\n",
    "    \"cl-tohoku/bert-base-japanese-char\": \"https://huggingface.co/cl-tohoku/bert-base-japanese-char/resolve/main/config.json\",\n",
    "    \"cl-tohoku/bert-base-japanese-char-whole-word-masking\": \"https://huggingface.co/cl-tohoku/bert-base-japanese-char-whole-word-masking/resolve/main/config.json\",\n",
    "    \"TurkuNLP/bert-base-finnish-cased-v1\": \"https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json\",\n",
    "    \"TurkuNLP/bert-base-finnish-uncased-v1\": \"https://huggingface.co/TurkuNLP/bert-base-finnish-uncased-v1/resolve/main/config.json\",\n",
    "    \"wietsedv/bert-base-dutch-cased\": \"https://huggingface.co/wietsedv/bert-base-dutch-cased/resolve/main/config.json\",\n",
    "    # See all BERT models at https://huggingface.co/models?filter=bert\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class BertConfig(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "    이 configuration 클래스는 transformers.BertModel 이나 transformers.TFBertModel 에 사용되는 규약을 저장해 놓은 것입니다. \n",
    "    버트 모델을 모델 구조를 정의하는 특정 argument 에 따라 인스턴스화 하는 데에 사용됩니다. \n",
    "    \n",
    "    This is the configuration class to store the configuration of a :class:`~transformers.BertModel` or a\n",
    "    :class:`~transformers.TFBertModel`. It is used to instantiate a BERT model according to the specified arguments,\n",
    "    defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration\n",
    "    to that of the BERT `bert-base-uncased <https://huggingface.co/bert-base-uncased>`__ architecture.\n",
    "\n",
    "    Configuration objects inherit from :class:`~transformers.PretrainedConfig` and can be used to control the model\n",
    "    outputs. Read the documentation from :class:`~transformers.PretrainedConfig` for more information.\n",
    "\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        vocab_size (:obj:`int`, `optional`, defaults to 30522):\n",
    "            Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\n",
    "            :obj:`inputs_ids` passed when calling :class:`~transformers.BertModel` or\n",
    "            :class:`~transformers.TFBertModel`.\n",
    "        hidden_size (:obj:`int`, `optional`, defaults to 768):\n",
    "            Dimensionality of the encoder layers and the pooler layer.\n",
    "        num_hidden_layers (:obj:`int`, `optional`, defaults to 12):\n",
    "            Number of hidden layers in the Transformer encoder.\n",
    "        num_attention_heads (:obj:`int`, `optional`, defaults to 12):\n",
    "            Number of attention heads for each attention layer in the Transformer encoder.\n",
    "        intermediate_size (:obj:`int`, `optional`, defaults to 3072):\n",
    "            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n",
    "        hidden_act (:obj:`str` or :obj:`Callable`, `optional`, defaults to :obj:`\"gelu\"`):\n",
    "            The non-linear activation function (function or string) in the encoder and pooler. If string,\n",
    "            :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` and :obj:`\"gelu_new\"` are supported.\n",
    "        hidden_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
    "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
    "        attention_probs_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
    "            The dropout ratio for the attention probabilities.\n",
    "        max_position_embeddings (:obj:`int`, `optional`, defaults to 512):\n",
    "            The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
    "            just in case (e.g., 512 or 1024 or 2048).\n",
    "        type_vocab_size (:obj:`int`, `optional`, defaults to 2):\n",
    "            The vocabulary size of the :obj:`token_type_ids` passed when calling :class:`~transformers.BertModel` or\n",
    "            :class:`~transformers.TFBertModel`.\n",
    "        initializer_range (:obj:`float`, `optional`, defaults to 0.02):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "        layer_norm_eps (:obj:`float`, `optional`, defaults to 1e-12):\n",
    "            The epsilon used by the layer normalization layers.\n",
    "        gradient_checkpointing (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> from transformers import BertModel, BertConfig\n",
    "\n",
    "        >>> # Initializing a BERT bert-base-uncased style configuration\n",
    "        >>> configuration = BertConfig()\n",
    "\n",
    "        >>> # Initializing a model from the bert-base-uncased style configuration\n",
    "        >>> model = BertModel(configuration)\n",
    "\n",
    "        >>> # Accessing the model configuration\n",
    "        >>> configuration = model.config\n",
    "    \"\"\"\n",
    "    model_type = \"bert\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=30522,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        pad_token_id=0,\n",
    "        gradient_checkpointing=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
