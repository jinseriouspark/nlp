{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # 위치적 인코딩 할 것 \n",
    "    # 실제 PE 를 구하는 식이 있음\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # dropout 할 비율을 정하는 것\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 0으로 이루어진 행렬을 max_len X d_model 형태로 만들어 pe 에 넣어둠\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # 위치는 0~ max_len 까지 원소를 만들고, max_len 길이의 행으로 변경\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # d_model 까지의 길이를 2 간격으로 숫자를 뽑아놓은 다음, 주어진 공식에 맞게 div_term 을 구함\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 짝수에는 sin()를, 홀수에는 cos() 함수를 취한다\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0,1)\n",
    "        self.register_buffer('pe', pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout = 0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        # super(모델명, self).__init__() 으로 한번 명시해줘야 함\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layer = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0,1)\n",
    "        # torch.ones(sz, sz) : 1로 구성된 행렬을 sz x sz 형태로 만듦\n",
    "        # torch.ones(sz, sz) == 1 : 각 원소가 1이라면 True, 아니라면 False\n",
    "        # torch.triu(torch.ones(sz, sz)) == 1 는 행렬의 윗 삼각형 부분만 건져냄(=true 로 만듦)\n",
    "        # .transpose(0,1) : 행렬 변환\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        # 윗 삼각형은 -inf 로 만들고, 아랫삼각형은 0으로 채워둠\n",
    "        return mask\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # 인코더의 가중치 초기값은 -initrange ~ initrange\n",
    "        self.decoder.bias.data.zeros_()\n",
    "        # 디코더의 bias 는 0으로 초기화\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # 디코더의 가중치 값도 동일하게 -initrange ~ initrange\n",
    "        \n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            # mask 가 none 이거나, 길이가 맞지 않을 때\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "        \n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask) \n",
    "        # src 와 src_mask 를 인코더에 넣어 결과물을 받는다\n",
    "        output = self.decoder(output) # 결과물을 디코더에 넣어 결과물을 얻는다\n",
    "        return output\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "positional encoding : 토큰의 절대적인 포지션을 주입함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 우선 dropout 을 할 p 값을 정해준다\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # d_model 은 인코더, 디코더에서 정해진 입력과 출력의 크기를 의미\n",
    "        # 논문에서는 512 개로 한정\n",
    "        # 5000개의 단어를 d_model 만큼 만들어 pe 행렬 생성\n",
    "        \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # 우선 0~ max_len까지의 수를 만든 후 행렬변환\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # 0~ d_model 까지 2개 차이나도록 만든 후 exp() 을 반영\n",
    "        # 이후 -4 / d_model 만큼, 입출력 크기 만큼 나누어준다\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 만들어진 pe 의 짝수열에는 position 의 sin값을 넣고\n",
    "        \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 만들어진 pe 의 홀수열에는 cos 값을 넣는단\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # 다시 행렬변환\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        # 그 pe 값을 x 값과 더한 결과를 순전파하는 것으로 사용한다\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 데이터 로드하고 배치 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"), \n",
    "                           init_token='<sos>',\n",
    "                           eos_token='<eos>',\n",
    "                           lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading wikitext-2-v1.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [13:39<00:00, 18.4KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n"
     ]
    }
   ],
   "source": [
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # 사용하고 있는 장비가 GPU or CPU 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # data.examples[0].text 는 띄어쓰기 처럼 문장을 나눈 단어로 구성\n",
    "    # .numericalize 를 통해 숫자로 변환\n",
    "    \n",
    "    # 데이터셋을 bsz파트들로 나눔\n",
    "    nbatch = data.size(0) // bsz\n",
    "     \n",
    "    # 깔끔하게 나누어 떨어지지 않는 추가적인 부분(나머지들)은 잘라낸다\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # narrow 는 주어진 차원에 따라 data 를 잘라내는 역할\n",
    "    # 0 차원, 0번째 원소부터 nbatch * bsz 길이까지만 data로 가져옴\n",
    "    \n",
    "    # 데이터에 대하여 bsz 배치들로 동등하게 나눕니다.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    # contiguous() 하면 실제 텐서의 복사본이 만들어짐\n",
    "    # 이는 narrow, view 등등, 텐서의 메모리를 공유하는 기능과 다르다\n",
    "    # 덕분에 메모리 상의 순서가 중요한 경우 이 복사 기능을 통해 오류를 방지할 수 있다\n",
    "    \n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
